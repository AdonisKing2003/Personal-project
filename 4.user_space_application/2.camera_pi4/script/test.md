# 1. Test camera with libcamera
```bash
root@raspberrypi4-64:~# libcamera-hello list-cameras
[0:17:39.870824143] [458]  INFO Camera camera_manager.cpp:327 libcamera v0.4.0+dirty (2025-12-07T15:39:35UTC)
[0:17:39.985680127] [459]  WARN RPiSdn sdn.cpp:40 Using legacy SDN tuning - please consider moving SDN inside rpi.denoise
[0:17:39.988998489] [459]  INFO RPI vc4.cpp:401 Registered camera /base/soc/i2c0mux/i2c@1/ov5647@36 to Unicam device /dev/media4 and ISP device /dev/media0
Preview window unavailable
Mode selection for 1296:972:12:P
    SGBRG10_CSI2P,640x480/0 - Score: 3296
    SGBRG10_CSI2P,1296x972/0 - Score: 1000
    SGBRG10_CSI2P,1920x1080/0 - Score: 1349.67
    SGBRG10_CSI2P,2592x1944/0 - Score: 1567
Stream configuration adjusted
[0:17:40.006543482] [458]  INFO Camera camera.cpp:1202 configuring streams: (0) 1296x972-YUV420 (1) 1296x972-SGBRG10_CSI2P
[0:17:40.008539788] [459]  INFO RPI vc4.cpp:570 Sensor: /base/soc/i2c0mux/i2c@1/ov5647@36 - Selected sensor format: 1296x972-SGBRG10_1X10 - Selected unicam format: 1296x972-pGAA
#0 (0.00 fps) exp 33239.00 ag 8.00 dg 1.00
#1 (30.01 fps) exp 33239.00 ag 8.00 dg 1.00
#2 (30.01 fps) exp 33239.00 ag 8.00 dg 1.00
#3 (30.01 fps) exp 33239.00 ag 8.00 dg 1.00
#4 (30.01 fps) exp 33239.00 ag 8.00 dg 1.00
#5 (30.01 fps) exp 33239.00 ag 8.00 dg 1.00
#6 (30.01 fps) exp 33239.00 ag 8.00 dg 1.00
#7 (30.01 fps) exp 33239.00 ag 8.00 dg 1.00
#8 (30.01 fps) exp 33239.00 ag 8.00 dg 1.00
...
```
:white_check_mark: Can detect camera


# 2. Test camera with cam -l
```bash
root@raspberrypi4-64:~# cam -l
[0:18:36.830680051] [467]  INFO Camera camera_manager.cpp:327 libcamera v0.4.0+dirty (2025-12-07T15:39:35UTC)
[0:18:36.871035709] [468]  WARN RPiSdn sdn.cpp:40 Using legacy SDN tuning - please consider moving SDN inside rpi.denoise
[0:18:36.873631415] [468]  INFO RPI vc4.cpp:401 Registered camera /base/soc/i2c0mux/i2c@1/ov5647@36 to Unicam device /dev/media4 and ISP device /dev/media0
Available cameras:
1: 'ov5647' (/base/soc/i2c0mux/i2c@1/ov5647@36)
...
```

:white_check_mark: Pi4 can recognize camera

# 3. Test gstreamer
```bash
root@raspberrypi4-64:~# gst-launch-1.0 videotestsrc ! videoconvert ! autovideosink
Setting pipeline to PAUSED ...
warning: queue 0x7fac000be0 destroyed while proxies still attached:
  xdg_wm_base@7 still attached
  wl_seat@6 still attached
  wl_subcompositor@5 still attached
  wl_compositor@4 still attached
  wl_registry@2 still attached
Pipeline is PREROLLING ...
Got context from element 'autovideosink0': gst.gl.GLDisplay=context, gst.gl.GLDisplay=(GstGLDisplay)"\(GstGLDisplayWayland\)\ gldisplaywayland0";
Pipeline is PREROLLED ...
Setting pipeline to PLAYING ...
Redistribute latency...
New clock: GstSystemClock
0:00:00.0 / 99:99:99.
^Chandling interrupt.
Interrupt: Stopping pipeline ...
Execution ended after 0:00:32.698699355
Setting pipeline to NULL ...
...
```

```bash
gst-launch-1.0 videotestsrc ! videoconvert ! autovideosink
```

- Uses videotestsrc, which is a synthetic pattern generator; it does not touch the Pi camera at all.
- autovideosink on a modern desktop usually picks a Wayland-capable sink, so any Wayland-related warnings are about the display backend only.
- The 0:00:00.0 / 99:99:99. and the clean shutdown after Ctrlâ€‘C are expected gst-launch-1.0 status messages.
â€‹
:white_check_mark: Gstreamer oke, but do not have **libcamerasrc**
ðŸ‘‰ videotestsrc ! videoconvert ! autovideosink only checks that GStreamer runs and can render video to your display; it never touches the Pi camera driver or libcamera stack.

# 4. Test v4l2
**1. Check v4l2**
```bash
root@raspberrypi4-64:~/workspace# v4l2-ctl --list-devices
bcm2835-codec-decode (platform:bcm2835-codec):
	/dev/video10
	/dev/video11
	/dev/video12
	/dev/video18
	/dev/video31
	/dev/media3

bcm2835-isp (platform:bcm2835-isp):
	/dev/video13
	/dev/video14
	/dev/video15
	/dev/video16
	/dev/video20
	/dev/video21
	/dev/video22
	/dev/video23
	/dev/media0
	/dev/media1

unicam (platform:fe801000.csi):
	/dev/video0
	/dev/media4

rpivid (platform:rpivid):
	/dev/video19
	/dev/media2
...
```
**2. Test v4l2**
```bash
root@raspberrypi4-64:~/workspace# gst-launch-1.0 v4l2src device=/dev/video0 ! videoconvert ! autovideosink
Setting pipeline to PAUSED ...
warning: queue 0x7f7c000be0 destroyed while proxies still attached:
  xdg_wm_base@7 still attached
  wl_seat@6 still attached
  wl_subcompositor@5 still attached
  wl_compositor@4 still attached
  wl_registry@2 still attached
Pipeline is live and does not need PREROLL ...
Got context from element 'autovideosink0': gst.gl.GLDisplay=context, gst.gl.GLDisplay=(GstGLDisplay)"\(GstGLDisplayWayland\)\ gldisplaywayland0";
Pipeline is PREROLLED ...
Setting pipeline to PLAYING ...
New clock: GstSystemClock
ERROR: from element /GstPipeline:pipeline0/GstV4l2Src:v4l2src0: Failed to allocate required memory.
Additional debug info:
/usr/src/debug/gstreamer1.0-plugins-good/1.22.12/sys/v4l2/gstv4l2src.c(950): gst_v4l2src_decide_allocation (): /GstPipeline:pipeline0/GstV4l2Src:v4l2src0:
Buffer pool activation failed
ERROR: from element /GstPipeline:pipeline0/GstV4l2Src:v4l2src0: Internal data stream error.
Additional debug info:
/usr/src/debug/gstreamer1.0/1.22.12/libs/gst/base/gstbasesrc.c(3134): gst_base_src_loop (): /GstPipeline:pipeline0/GstV4l2Src:v4l2src0:
streaming stopped, reason not-negotiated (-4)
Execution ended after 0:00:00.055619996
Setting pipeline to NULL ...
warning: queue 0x7f7c0dae30 destroyed while proxies still attached:
  xdg_wm_base@13 still attached
  wl_seat@12 still attached
  wl_subcompositor@11 still attached
  wl_compositor@10 still attached
  wl_registry@14 still attached
Freeing pipeline ...
...
```

hits memory/bufferâ€‘pool negotiation errors because /dev/video0 in this libcameraâ€‘based stack is not a simple UVCâ€‘style capture node designed for direct use by applications like GStreamer.

The unicam node expects to be driven as part of the libcamera pipeline, not by arbitrary userâ€‘space buffer allocation.

That is why you see Failed to allocate required memory and not-negotiated; GStreamerâ€™s v4l2src does not match what the driver/libcamera expect.
â€‹
So:

Yes, libcamera-still ultimately talks through the same hardware path that /dev/video0 represents.

But no, libcamera-still is not just â€œa thin wrapper over /dev/video0â€ in the way v4l2src is; it uses libcameraâ€™s full pipeline and controls, while v4l2src tries to use /dev/video0 directly and fails on this modern stack.
â€‹
:warning: **Error**

# 5. Get image from camera using libcamera
```bash
libcamera-still -n -t 1000 -o test.jpg
```

-n disables preview (no window).
-t 1000 makes it run for 1 second.
:warning:**Let it finish by itself; do not press Ctrlâ€‘C. Then check for test.jpg in the current directory.**



# 6. Stream from pi to laptop

**1. Setup On ubbuntu**

```bash
sudo apt update
sudo apt install \
  gstreamer1.0-plugins-base \
  gstreamer1.0-plugins-good \
  gstreamer1.0-plugins-bad \
  gstreamer1.0-plugins-ugly \
  gstreamer1.0-libav
```

**2. Start stream on pi4:**
```bash
root@raspberrypi4-64:~/workspace# libcamera-vid -t 0   --width 1280 --height 720 --framerate 25   --codec h264 --bitrate 2000000 -o - | gst-launch-1.0 -v fdsrc ! h264parse ! rtph264pay config-interval=1 pt=96 ! udpsink host=10.42.0.1 port=5000
```
**3. Get stream on ubuntu:**
```bash
adonisking@adonisking-Nitro-AN515-55:~$ gst-launch-1.0 -v udpsrc port=5000 caps="application/x-rtp,media=video,encoding-name=H264,payload=96" ! rtph264depay ! avdec_h264 ! videoconvert ! autovideosink
```
---> Test OK, can stream


# 7. Test camera app
```bash
root@raspberrypi4-64:~/workspace# ./camera_test 
malloc(): corrupted top size
Aborted (core dumped)
```
Summary of the Camera Issue from Beginning to End
The Problem Journey
1. Initial Problem: VIDIOC_STREAMON Failed
You had V4L2 code that worked on other systems but failed on Raspberry Pi 4 with this error:
[ERROR]: STREAMON: Invalid argument
Root Cause: Raspberry Pi uses the unicam driver, which is part of a complex media pipeline that requires proper configuration before streaming.
2. Why Standard V4L2 Doesn't Work on Raspberry Pi
Your code was trying to use standard V4L2 API:
ccamera->fd = open("/dev/video0", O_RDWR);
ioctl(camera->fd, VIDIOC_S_FMT, &fmt);
ioctl(camera->fd, VIDIOC_STREAMON, &type);  // âŒ This fails!
```

**The kernel logs showed:**
```
unicam fe801000.csi: Failed to start media pipeline: -22
This happens because the Raspberry Pi camera architecture requires:

Media Controller configuration (linking sensor â†’ unicam)
Sensor subdevice format setup
Pipeline validation before streaming

Even v4l2-ctl failed with the same error, proving it wasn't your code - it's the driver design.
3. Why the Media Pipeline Setup Didn't Help
We tried configuring the media pipeline manually:
bashmedia-ctl -d /dev/media3 -l "'ov5647 10-0036':0 -> 'unicam-image':0[1]"
media-ctl -d /dev/media3 -V "'ov5647 10-0036':0 [fmt:SGBRG10_1X10/640x480]"
This still failed because the unicam driver has additional internal requirements that aren't exposed through standard V4L2 ioctls.
4. Why Legacy Driver Didn't Work
We tried enabling the old bcm2835-v4l2 driver, but modern Raspberry Pi OS (based on kernel 6.6) has removed legacy camera support in favor of libcamera.

The Solution: Using libcamera Without Headers
Why This Approach Works
Instead of using V4L2 directly or importing libcamera headers, we use system calls to libcamera command-line tools that are already installed on the Raspberry Pi.
How It Works
The Raspberry Pi comes with libcamera-apps pre-installed, which includes:

libcamera-still - Capture still images
libcamera-vid - Capture video
libcamera-jpeg - Capture JPEG images

These tools handle ALL the complex pipeline setup internally:

They open the media controller
Configure the sensor subdevice
Set up the unicam driver
Start the pipeline
Capture frames

Your code just calls them via system() or popen() - no headers needed!

The Simple C Implementation Explained
Method 1: Capture a Single Image
```c
int capture_image_simple(const char *output_file) {
    char cmd[512];
    
    // Build the command string
    snprintf(cmd, sizeof(cmd), 
             "libcamera-still -o %s --width 640 --height 480 --timeout 1 --nopreview 2>/dev/null",
             output_file);
    
    // Execute the command
    int ret = system(cmd);
    
    if (ret != 0) {
        fprintf(stderr, "Failed to capture image\n");
        return -1;
    }
    
    printf("Image captured to: %s\n", output_file);
    return 0;
}
```
What happens:

system() spawns a shell and runs libcamera-still
libcamera-still does all the camera setup internally
It captures one frame and saves it to the file
Returns 0 on success

Usage:
```c
capture_image_simple("photo.jpg");
// Now you have photo.jpg you can read/process
```
Method 2: Capture Video to File
```c
int capture_video_frames(const char *output_file, int duration_ms) {
    char cmd[512];
    
    snprintf(cmd, sizeof(cmd),
             "libcamera-vid -o %s --width 640 --height 480 -t %d --nopreview --codec yuv420 2>/dev/null",
             output_file, duration_ms);
    
    int ret = system(cmd);
    return (ret == 0) ? 0 : -1;
}
```
What happens:

libcamera-vid captures video for the specified duration
Saves raw YUV420 frames to a file
You can then read the file and process frames

Usage:
```c
capture_video_frames("video.yuv", 2000);  // 2 seconds
// Now you have video.yuv with raw frame data
```
Method 3: Stream Frames Continuously (Most Useful)
```c
typedef struct {
    FILE *pipe;           // Pipe to libcamera-vid process
    int width;
    int height;
    size_t frame_size;    // Size of one frame in bytes
} camera_stream_t;

camera_stream_t* camera_stream_start(int width, int height) {
    camera_stream_t *stream = malloc(sizeof(camera_stream_t));
    
    stream->width = width;
    stream->height = height;
    stream->frame_size = width * height * 3 / 2;  // YUV420: 1.5 bytes per pixel
    
    char cmd[512];
    snprintf(cmd, sizeof(cmd),
             "libcamera-vid --width %d --height %d -t 0 --nopreview "
             "--codec yuv420 -o - 2>/dev/null",  // "-o -" means output to stdout
             width, height);
    
    // popen() runs the command and gives us a pipe to read from
    stream->pipe = popen(cmd, "r");
    
    return stream;
}
```
What happens:

popen() starts libcamera-vid as a child process
-o - tells it to write frames to stdout instead of a file
-t 0 means run indefinitely
We get a FILE* pipe we can read from

Reading frames:
```c
int camera_stream_read_frame(camera_stream_t *stream, uint8_t *buffer) {
    // Read exactly one frame worth of data
    size_t bytes_read = fread(buffer, 1, stream->frame_size, stream->pipe);
    
    if (bytes_read != stream->frame_size) {
        return -1;  // Error or end of stream
    }
    
    return 0;  // Success - buffer now contains one YUV420 frame
}
```
Complete usage example:
```c
// Start streaming
camera_stream_t *stream = camera_stream_start(640, 480);

// Allocate buffer for one frame
uint8_t *frame = malloc(stream->frame_size);

// Capture 100 frames
for (int i = 0; i < 100; i++) {
    if (camera_stream_read_frame(stream, frame) == 0) {
        // Process frame data here
        // frame[0...307199] contains YUV420 pixel data
        printf("Frame %d captured\n", i);
    }
}

// Cleanup
free(frame);
camera_stream_stop(stream);
```

---

## Why This Solution is Better

### âœ… Advantages

1. **No cross-compilation issues** - Uses standard C libraries only
2. **No headers needed** - No libcamera development files required
3. **Works on any Pi** - Relies on pre-installed libcamera-apps
4. **Simple to understand** - Just shell commands wrapped in C
5. **Handles all complexity** - libcamera-vid does the hard work

### âš ï¸ Limitations

1. **Process overhead** - Spawns external process (uses ~10-20ms extra)
2. **Less control** - Can't access advanced camera features easily
3. **Format limited** - Mainly YUV420 or JPEG
4. **Latency** - Small delay from pipe communication

---

## Frame Data Format (YUV420)

When you read a frame, you get raw YUV420 data:
```bash
For 640x480:
- Y plane:  640 Ã— 480 = 307,200 bytes (luminance)
- U plane:  320 Ã— 240 =  76,800 bytes (chrominance)
- V plane:  320 Ã— 240 =  76,800 bytes (chrominance)
Total:                  460,800 bytes (640Ã—480Ã—1.5)
```
To convert to RGB or process:
```c
// Y starts at offset 0
uint8_t *Y = frame;

// U starts after Y
uint8_t *U = frame + (width * height);

// V starts after U
uint8_t *V = frame + (width * height) + (width * height / 4);

// Convert YUV to RGB (simplified)
for (int i = 0; i < width * height; i++) {
    int y = Y[i];
    int u = U[i/4] - 128;
    int v = V[i/4] - 128;
    
    int r = y + 1.402 * v;
    int g = y - 0.344 * u - 0.714 * v;
    int b = y + 1.772 * u;
    
    // Clamp and use r, g, b values...
}
```

Summary
The Problem: Standard V4L2 doesn't work on Raspberry Pi due to complex media pipeline requirements.
The Solution: Use libcamera tools via system calls - no headers, no cross-compilation issues, works perfectly.
The Trade-off: Slightly less control and small overhead, but dramatically simpler implementation.
This is the most practical solution for cross-compiled applications that need camera access on Raspberry Pi!Claude is AI and can make mistakes. Please double-check responses.

### result:
root@raspberrypi4-64:~/workspace# ./camera_test 
=== Libcamera Simple Camera Test ===

Example 1: Capturing single image...
Capturing image...
Image captured to: test_image.jpg
Success!

Example 2: Capturing 2 second video...
Capturing video...
Video captured to: test_video.yuv
Success!

Example 3: Streaming frames for 5 seconds...
Camera stream started: 640x480
Captured frame 1
Saved frame_001.yuv
Captured frame 119
Total frames captured: 119
Camera stream stopped

All tests completed!
root@raspberrypi4-64:~/workspace# 

## video weird
Ah! So still images work fine but video streaming has artifacts. This is a different issue - likely a frame synchronization problem.
The Problem: Frame Alignment
When streaming YUV data continuously, you might be reading in the middle of a frame, causing the frames to be misaligned. This creates the color distortion you see.
Solution: Add Frame Synchronization
The issue is that fread() might return partial data, and you continue reading, which shifts all subsequent frames.
Fix 1: Ensure Complete Frame ReadsCamera Capture with libcamera C APICodeâ€‚âˆ™â€‚Version 4Â int camera_stream_read_frame(camera_stream_t *stream, uint8_t *buffer) {
    if (!stream || !stream->pipe || !buffer) {
        return -1;
    }
    
    // Read frame in chunks to ensure we get complete data
    size_t total_read = 0;
    while (to

Fix 2: Use Larger Pipe Buffer
Alternatively, configure the pipe with a larger buffer to reduce partial reads:Camera Capture with libcamera C APICodeâ€‚âˆ™â€‚Version 5Â camera_stream_t* camera_stream_start(int width, int height) {
    camera_stream_t *stream = malloc(sizeof(camera_stream_t));
    if (!stream) {
        return NULL;
    }
    
    stream->width = width;
    stream->height = height;
    stream->frame_

Fix 3: Save Multiple Frames and Check Them
Add debug code to save several frames and check if they're all corrupted or just some:
c// In your main loop, save first 10 frames
if (frame_count <= 10) {
    char filename[64];
    snprintf(filename, sizeof(filename), "frame_%03d.yuv", frame_count);
    FILE *fp = fopen(filename, "wb");
    if (fp) {
        fwrite(frame_buffer, 1, stream->frame_size, fp);
        fclose(fp);
        
        // Convert to JPEG immediately
        char cmd[256];
        snprintf(cmd, sizeof(cmd),
                "ffmpeg -f rawvideo -pixel_format yuv420p -video_size 640x480 "
                "-i %s %s.jpg -y 2>/dev/null",
                filename, filename);
        system(cmd);
    }
}
Then check:
bashls frame_*.jpg
# View each one
for f in frame_*.jpg; do echo $f; xdg-open $f; sleep 1; done
Alternative: Use Named Pipe (FIFO) for Better Buffering
This gives you more control:
c#include <sys/stat.h>
#include <fcntl.h>

camera_stream_t* camera_stream_start(int width, int height) {
    camera_stream_t *stream = malloc(sizeof(camera_stream_t));
    if (!stream) return NULL;
    
    stream->width = width;
    stream->height = height;
    stream->frame_size = width * height * 3 / 2;
    
    // Create named pipe
    const char *fifo_path = "/tmp/camera_fifo";
    unlink(fifo_path);  // Remove if exists
    if (mkfifo(fifo_path, 0666) != 0) {
        perror("mkfifo failed");
        free(stream);
        return NULL;
    }
    
    // Start libcamera-vid writing to FIFO
    char cmd[512];
    snprintf(cmd, sizeof(cmd),
             "libcamera-vid --width %d --height %d -t 0 --nopreview "
             "--codec yuv420 --inline -o %s 2>/dev/null &",
             width, height, fifo_path);
    
    system(cmd);
    sleep(1);
    
    // Open FIFO for reading
    int fd = open(fifo_path, O_RDONLY);
    if (fd < 0) {
        perror("Failed to open FIFO");
        free(stream);
        return NULL;
    }
    
    stream->pipe = fdopen(fd, "r");
    if (!stream->pipe) {
        close(fd);
        free(stream);
        return NULL;
    }
    
    // Skip first frames
    uint8_t *dummy = malloc(stream->frame_size);
    for (int i = 0; i < 5; i++) {
        read(fd, dummy, stream->frame_size);
    }
    free(dummy);
    
    return stream;
}
Try the updated code first (with complete frame reads). If that doesn't fix it, please show me:

Output from the program showing how many bytes each frame reads
A few consecutive frames converted to JPEG so I can see if the corruption pattern changes
